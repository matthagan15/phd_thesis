\chapter{Introduction}

One of the most challenging issues facing theoretical physicists, chemists, and materials scientists throughout the 20th and 21st centuries is the problem of scale. Throughout a traditional physics education one encounters simple systems that can be solved by hand, but whenever one enters research this quickly breaks down. It is humanly impossible to compute the quantum energy levels for a system by hand at the macroscopic scale, say an Avogadro's number of particles which is on the order of $10^{-23}$. Science has organized itself primarily by the tools used to address various scales: high energy theorists work closest ``to the metal'', chemists have a myriad of techniques to reason about the complicated dynamics at play in large, irregular compounds, and condensed matter theorists have developed tools for macroscopic systems with regularities and symmetries, such as metals and crystals. This observation that the scale of a problem leads to qualitatively different techniques was first observed by Nobel laureate Paul Anderson in his article ``More is Different'' \cite{moreIsDifferent}.

One of the primary tools for tackling this issue of scale has been classical computers. For all of the heroic efforts of high energy physicsists, the Higgs Boson would not have been discovered without the machine learning techniques that filter through the vast majority of collected data from the LHC to find the outliers that contain the information that leads to discoveries. The human genome would not have been mapped without algorithmic advances that allow for accurately reconstructing large genetic sequences from small splices. Gravitational waves could not have been experimentally observed without the numerical solutions to Einstein's field equations that were used to predict what the signal of two colliding black holes would look like. In fact, whenever the first gravitational waves were discovered, virtually right after the experiment entered an observation mode from an upgrade, the LIGO team spent months with an external auditor to confirm that their systems had not been hacked, which they viewed as more likely at the time. 

The real promise of quantum computers is they offer a way to drastically improve the numerical simulation of quantum systems at scale. Although quantum systems can be studied using classical computers the algorithms to do so tend to require an exponential increase in the resources needed with the number of particles being simulated. For example, one can classically simulate a single trajectory in a path integral fairly efficiently if the Hamiltonian is efficiently computable. However one typically encounters the sign problem where the phase of each path must be computed and an exponential number of paths must be sampled from to accurately estimate a quantity of interest. Quantum computers avoid this by directly simulating the system of interest on a quantum device that can be controlled with exquisite precision. This then allows us to evolve the system over time and to estimate quantities of interest directly. 

This thesis is centered around the development of algorithms to implement time evolution and cooling processes on a fault-tolerant digital quantum computer. 

To simplify the computational model we will assume access to a noise and decoherence free quantum computer, typically called a fault-tolerant device. This allows us to abstract away any specific hardware and error-correction platform and focus solely on the computational task of guaranteeing an accurate quantum simulation. The main problems encountered in simulating quantum systems are preparing initial states and performing the time evolution operator $U = e^{i H t}$ given access to $H$. This thesis develops and studies two new algorithms for tackling these problems, providing extensive analytic and numeric evidence that investigates their performance. The rest of this section will develop the tools needed for these topics in technical detail. 