\chapter{Introduction}

\textbf{Attempt 2}
\section{Motivation}
This thesis is an attempt to understand a quantum analog of the Hamiltonian Monte Carlo (HMC) algorithm. If you are interested in the results of this experiment I will do my best to explain them starting from the beginning. This journey will involve an explanation of the original Metropolis-Hastings algorithm, the Hamiltonian modification that yields HMC, how to simulate weak interactions with an environment, and finally how to utilize these to prepare thermal states, the quantum version of the output of HMC. The last chapter of this thesis includes numerics that reinforce the analytic results. 

Our starting point is the Metropolis-Hastings algorithm. This algorithm was developed to address the fundamental problem facing scientists of ``scale''. As physics offers the most accurate underlying theory of microscopic interactions, we start there. When learning physics, the most common teaching tools are small, toy problems. A block on a perfectly triangular incline plane. A ball attached to the end of a very long pendulum that is barely moved from it's resting point. Although these problems are not 100\% realistic, they offer great starting points to add more realistic features. Maybe curvature is added to the incline plane or the oscillator is damped. We can make these toy problems more realistic in this manner, and typically they can be solved via perturbative methods or other various approximations. The more challenging issue when making these toy problems more realistic is the problem of scale. We can solve a single mass and spring, but how do we solve $10^{-23}$ masses and springs all connected together? 

This problem of scale leads to fundamentally different tools and techniques being used at various levels. The methods used by high energy theorists working ``closest to the metal'' are going to be radically different, but no more valid, than the techniques of climate scientists, which are different than cosmologists. This was first observed by Nobel laureate Paul Anderson with the philosophy ``More is Different'' \cite{moreIsDifferent}. One of the primary tools theorists have to address this difficulty of scale are classical computers. The reason computers allow us to transcend many orders of magnitude when solving problems is that they are simply much faster at basic arithmetic, namely floating point operations, than people. For example, at the time of writing the world record for the longest number memorized is 3260 digits memorized and recalled perfectly given an hour to commit the number to memory. A single Apple watch can store 700,000 equally long numbers and perform basic arithmetic on each one in less than a second. The sheer scale of leading supercomputer clusters is difficult to comprehend or put in a human perspective. 

One of the oldest and most studied algorithm for physicists, chemists, or material scientists to take advantage of the absurdly large computational power of classical compute clusters is the Metropolis-Hastings algorithm \cite{metropolis1953equation}. This algorithm is a general purpose tool that allows one to sample from thermal distributions of a given classical Hamiltonian, and as such has given scientists a computational lens to understanding the behavior of large systems. However, one of the downsides to the algorithm is that it exhibits ``random walk'' behavior, meaning it explores the state space in a diffusive manner, leading to a scaling of $D^2$ where $D$ is the dimension of your dataset. To avoid this random walk, better techniques have been proposed that take advantage of gradient information to mimic Hamiltonian time evolution which allows one to traverse larger distances before generating a sample, which leads to less correlation.

On the physics side of the coin, these algorithms essentially prepare samples from the Boltzmann distribution $p_\beta (\vec{x}) = \frac{e^{-\beta H(\vec{x})}}{\int e^{-\beta H(\vec{x})} d\vec{x}}$. This is the generic distribution that describes the ``canonical'' ensemble, or the probability distribution that systems at a fixed temperature exhibit. This then allows us to estimate observables $O$ of systems at a given temperature $\beta$ as $\langle O \rangle = \int O(\vec{x}) p_\beta (\vec{x}) d\vec{x}$. As an example, if we would like to numerically explore what the average magnetization of a given compound is then the observable we are interested would be the average of all the component spins $O = \frac{1}{n} \sum_{i = 1}^n \hat{S}_i$. 

From a physics perspective, we have a pretty good idea of when classical systems tend to thermalize, or when their state is very close to the thermal state. We now that when in contact with a large bath at inverse temperature $\beta$ in the long time limit as $t \to \infty$ we get the thermal state for the system. For quantum mechanics the picture is much less clear. There exists models that mimic weak interactions with a large, memoryless bath that lead to systems thermalizing. One example of this are the Davies Generators \cite{davies1974markovian} which can lead to thermalization for arbitrary systems (I think) in the infinite time, zero coupling limit. Further confusing the picture is whether one wants to study an open quantum system, in which the environment is only important because of the transitions it can provide for the system, or a closed quantum system, in which a large system only interacts with itself and no external transfer of energy is present. Our best understanding of whenever a closed system can appear thermalized to a small observable is the Eigenstate Thermalization Hypothesis (ETH), which works for Hamiltonians that appear sufficiently random but not for all Hamiltonians (hence the name Eigenstate Thermalization \emph{Hypothesis}, not Theorem). 

This thesis is concerned with an intermediate regime for thermalization models. In the Davies Generator picture we are modeling a nearly infinite sized environment solely via the transitions it induces in the system and in the ETH picture we have no environment whatsoever. The thermalization model that we end up with has a very small environment, in our scenario a single qubit, that gets refreshed many times. This model is known as the Repeated Interactions (RI) model and is previously only studied for very small systems, namely 2 or 3 energy levels. In this thesis we will demonstrate how this model can be extended to an arbitrary non-degenerate system and the conditions dictating non-degeneracy are very weak and can most likely be lifted.

We will work our way up to this main contribution. To do so, we will first explore how we can improve the main quantum subroutine used, time independent Hamiltonian simulation.

% \textbf{Attempt 1}

% One of the most challenging issues facing theoretical physicists, chemists, and materials scientists throughout the 20th and 21st centuries is the problem of scale. Throughout a traditional physics education one encounters simple systems that can be solved by hand, but whenever one enters research this quickly breaks down. It is humanly impossible to compute the quantum energy levels for a system by hand at the macroscopic scale, say an Avogadro's number of particles which is on the order of $10^{-23}$. Science has organized itself primarily by the tools used to address various scales: high energy theorists work closest ``to the metal'', chemists have a myriad of techniques to reason about the complicated dynamics at play in medium sized, irregular compounds, and condensed matter theorists have developed tools for macroscopic systems with regularities and symmetries, such as metals and crystals. This observation that the scale of a problem leads to qualitatively different techniques that are in no way less ``fundamental'' than those at lower complexity levels was first observed by Nobel laureate Paul Anderson in his article ``More is Different'' \cite{moreIsDifferent}.

% This separation of complexity scale seems to be a byproduct of the limited working memory of humans; the average person can hold only 7 or so numeric or phonetic ``chunks'' in their mind at once. Given extensive training of this capacity, and an hour to commit a number to memory, the current world record is 3260 digits memorized and recalled perfectly. The most recent Apple watch in 2025 can store about 700,000 numbers with 3260 digits in memory and execute basic arithmetic on every single one in less than a minute, if not a second.
% % An Apple watch can not only store 134,217,728 64 bit integers (each integer has about 19 decimal digits) and perform basic arithmetic on every single one in less than a second. 
% This sheer difference in computational power for simple tasks has made classical computers fundamental in slashing the complexity gap for some problems. A classic example of this is the fantastic ability of large data and supercomputers to approximately solve nonlinear differential equations to give a 3-5 day weather prediction. However, even the best models we have tend to fail beyond this short window due to the chaotic nature of the differential equation. To get longer term behavior, such as changes in global climate compared to local weather, the model needs to be ``coarse-grained'' across the spatial component. It is easier to predict the average global temperature in 5 years than the specific temperature of a city. 

% The promise of quantum computers is they offer a way to drastically improve the numerical simulation of quantum systems at scale. Although quantum systems can be studied using classical computers the algorithms to do so tend to require an exponential increase in the resources needed with the number of particles being simulated. For example, one can classically simulate a single trajectory in a path integral fairly efficiently if the Hamiltonian is efficiently computable. However one typically encounters the sign problem where the phase of each path must be computed and an exponential number of paths must be sampled from to accurately estimate a quantity of interest. Quantum computers avoid this by directly simulating the system of interest on a quantum device that can be controlled with exquisite precision. This then allows us to evolve the system over time and to estimate quantities of interest directly. 

% To simplify the computational model we will assume access to a noise and decoherence free quantum computer, typically called a fault-tolerant device. This allows us to abstract away any specific hardware and error-correction platform and focus solely on the computational task of guaranteeing an accurate quantum simulation. The main problems encountered in simulating quantum systems are preparing initial states and performing the time evolution operator $U = e^{i H t}$ given access to $H$. This thesis develops and studies two new algorithms for tackling these problems, providing extensive analytic and numeric evidence that investigates their performance. The rest of this section will develop the tools needed for these topics in technical detail. 

% \textbf{Option 1:\\}
% One of the primary tools for tackling this issue of scale has been classical computers. For all of the heroic efforts of high energy physicsists, the Higgs Boson would not have been discovered without the machine learning techniques that filter through the vast majority of collected data from the LHC to find the unique signature of the Higgs. The human genome would not have been mapped without algorithmic advances that allow for accuratsely reconstructing large genetic sequences from massive amounts of small splices. Gravitational waves could not have been experimentally observed without the numerical solutions to Einstein's field equations that tell us what the signal of two colliding black holes actually looks and sounds like. In fact, whenever the first gravitational waves were discovered, which was almost immediately after the experiment entered an observation mode from an upgrade, the LIGO team spent months with an external auditor to confirm that their computer systems had not been hacked, which they viewed as more likely at the time. The impact of classical computers on science is difficult to overstate. 