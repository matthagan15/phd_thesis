\chapter{Introduction}

One of the most challenging issues facing theoretical physicists, chemists, and materials scientists throughout the 20th and 21st centuries is the problem of scale. Throughout a traditional physics education one encounters simple systems that can be solved by hand, but whenever one enters research this quickly breaks down. It is humanly impossible to compute the quantum energy levels for a system by hand at the macroscopic scale, say an Avogadro's number of particles which is on the order of $10^{-23}$. Science has organized itself primarily by the tools used to address various scales: high energy theorists work closest ``to the metal'', chemists have a myriad of techniques to reason about the complicated dynamics at play in medium sized, irregular compounds, and condensed matter theorists have developed tools for macroscopic systems with regularities and symmetries, such as metals and crystals. This observation that the scale of a problem leads to qualitatively different techniques that are in no way less ``fundamental'' than those at lower complexity levels was first observed by Nobel laureate Paul Anderson in his article ``More is Different'' \cite{moreIsDifferent}.

This separation of complexity scale seems to be a byproduct of the limited working memory of humans; the average person can hold only 7 or so numeric or phonetic ``chunks'' in their mind at once. Given extensive training of this capacity, and an hour to commit a number to memory, the current world record is 3260 digits memorized and recalled perfectly. An Apple watch can not only store 134,217,728 64 bit integers (each integer has about 19 decimal digits) and perform basic arithmetic on every single one in less than a second. This sheer difference in computational power for simple tasks has made classical computers fundamental in slashing the complexity gap for some problems. A classic example of this is the fantastic ability of large data and supercomputers to approximately solve nonlinear differential equations to give a 3-5 day weather prediction. However, even the best models we have tend to fail beyond this short window due to the chaotic nature of the differential equation. To get longer term behavior, such as changes in global climate compared to local weather, the model needs to be ``coarse-grained'' across the spatial component. It is easier to predict the average global temperature in 5 years than the specific temperature of a city. 



Quantum computers offer a completely unique tool to allow for scientists to cut through the complexity of scale by simulating exotic quantum mechanical systems on quantum devices that we can explicitly control. The difficulty of classical computers in simulating quantum systems is well observed but not completely understood. 

Quantum computers offer a completely unique tool to allow physicists, chemists, and material scientists a new approach of reducing some, but not all, of the issues of scale for large quantum systems. To illustrate this advantage we will walk through a typical electronic structure problem, where one wants to know what the ground state energy of the electrons is for a given configuration of nuclei in some chemical compound. This can then be used by chemists to determine if a given chemical reaction will occur and at what rate. For some systems, such as some large organic compounds, the electrons are very loosely correlated and can be modeled accurately with classical techniques. Quantum computers are of no help for these types of systems. For systems in which electrons exhibit strong correlation, classical methods tend to break down and fail to converge due to the fermionic sign problem. This is where the most exotic materials are typically found, for example in 

% The promise of quantum computers is they offer a way to drastically improve the numerical simulation of quantum systems at scale. Although quantum systems can be studied using classical computers the algorithms to do so tend to require an exponential increase in the resources needed with the number of particles being simulated. For example, one can classically simulate a single trajectory in a path integral fairly efficiently if the Hamiltonian is efficiently computable. However one typically encounters the sign problem where the phase of each path must be computed and an exponential number of paths must be sampled from to accurately estimate a quantity of interest. Quantum computers avoid this by directly simulating the system of interest on a quantum device that can be controlled with exquisite precision. This then allows us to evolve the system over time and to estimate quantities of interest directly. 

This thesis is centered around the development of algorithms to implement time evolution and cooling processes on a fault-tolerant digital quantum computer. 

To simplify the computational model we will assume access to a noise and decoherence free quantum computer, typically called a fault-tolerant device. This allows us to abstract away any specific hardware and error-correction platform and focus solely on the computational task of guaranteeing an accurate quantum simulation. The main problems encountered in simulating quantum systems are preparing initial states and performing the time evolution operator $U = e^{i H t}$ given access to $H$. This thesis develops and studies two new algorithms for tackling these problems, providing extensive analytic and numeric evidence that investigates their performance. The rest of this section will develop the tools needed for these topics in technical detail. 

% \textbf{Option 1:\\}
% One of the primary tools for tackling this issue of scale has been classical computers. For all of the heroic efforts of high energy physicsists, the Higgs Boson would not have been discovered without the machine learning techniques that filter through the vast majority of collected data from the LHC to find the unique signature of the Higgs. The human genome would not have been mapped without algorithmic advances that allow for accuratsely reconstructing large genetic sequences from massive amounts of small splices. Gravitational waves could not have been experimentally observed without the numerical solutions to Einstein's field equations that tell us what the signal of two colliding black holes actually looks and sounds like. In fact, whenever the first gravitational waves were discovered, which was almost immediately after the experiment entered an observation mode from an upgrade, the LIGO team spent months with an external auditor to confirm that their computer systems had not been hacked, which they viewed as more likely at the time. The impact of classical computers on science is difficult to overstate. 